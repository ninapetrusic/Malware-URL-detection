from sklearn import tree
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn import preprocessing
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import sys
sys.path.append('..')
import contentFeatures
import lexicalFeatures
import hostFeatures

## unique hosts
cache = {}

data_mal = pd.read_csv('FinalDataset/URL/Malware_dataset.csv')
data_mal['class'] = 'malware'

data_ben = pd.read_csv('FinalDataset/URL/Benign_list_big_final.csv')
data_ben['class'] = 'benign'

##data = pd.concat([data_mal.head(400) ,data_mal.tail(400), data_ben.head(400), data_ben.tail(400)], axis=0)
data = pd.concat([data_mal.iloc[1601:2001], data_ben.iloc[1601:2001]], axis=0)
data_copy = data.copy()

for i,row in data_copy.iterrows():
    print('------%d' % (i))
    cf = contentFeatures.contentFeatures(row['url'])
    ## content
    data.at[i, 'pageEntropy'] = cf.pageEntropy()
    data.at[i, 'numOfScripts'] = cf.numOfScripts()
    data.at[i, 'scriptBodyRatio'] = cf.scriptBodyRatio()
    data.at[i, 'htmlLen'] = cf.htmlLen()
    data.at[i, 'numOfHiddenTags'] = cf.numOfHiddenTags()
    data.at[i, 'numIframes'] = cf.numIframes()
    data.at[i, 'numEmbeds'] = cf.numEmbeds()
    data.at[i, 'numObjects'] = cf.numObjects()
    data.at[i, 'numLinks'] = cf.numLinks()
    data.at[i, 'numTags'] = cf.numTags()
    data.at[i, 'numIncludedElements'] = cf.numIncludedElements()
    data.at[i, 'numWhitespaces'] = cf.numWhitespaces()
    data.at[i, 'numEvalFunc'] = cf.numEvalFunc()
    data.at[i, 'numSentences'] = cf.numSentences()
    data.at[i, 'avgScriptLen'] = cf.avgScriptLen()
    data.at[i, 'avgScriptEntropy'] = cf.avgScriptEntropy()
    ## lexical
    lf = lexicalFeatures.lexicalFeatures(row['url'])
    data.at[i, 'numDigits'] = lf.numDigits()
    data.at[i, 'urlLength'] = lf.urlLength()
    data.at[i, 'pathLength'] = lf.pathLength()
    data.at[i, 'hostLength'] = lf.hostLength()
    data.at[i, 'numParameters'] = lf.numParameters()
    data.at[i, 'numFragments'] = lf.numFragments()
    data.at[i, 'urlProtocol'] = lf.urlProtocol()
    data.at[i, 'hostIsIP'] = lf.hostIsIP()
    data.at[i, 'entropy'] = lf.entropy()
    data.at[i, 'subdirs'] = lf.subdirs()
    data.at[i, 'client'] = lf.client()
    data.at[i, 'login'] = lf.login()
    data.at[i, 'admin'] = lf.admin()
    data.at[i, 'server'] = lf.server()
    data.at[i, 'periods'] = lf.periods()
    data.at[i, 'isEncoded'] = lf.isEncoded()
    ## TODO host
    hf = hostFeatures.hostFeatures(row['url'],cache)
    data.at[i, 'urlAge'] = hf.urlAge()
    data.at[i, 'urlIntendedLifeSpan'] = hf.urlIntendedLifeSpan()
    data.at[i, 'ttl'] = hf.ttl()
    data.at[i, 'totalUpdates'] = hf.totalUpdates()
    data.at[i, 'connectionSpeed'] = hf.connectionSpeed()
    data.at[i, 'daysSinceLastSeen'] = hf.daysSinceLastSeen()
    data.at[i, 'firstSeen'] = hf.firstSeen()
    data.at[i, 'urlHostIsIP'] = hf.urlHostIsIP()
    data.at[i, 'numOfSubdomains'] = hf.numOfSubdomains()
    data.at[i, 'urlRegCountry'] = hf.urlRegCountry()
    data.at[i, 'urlHostCountry'] = hf.urlHostCountry()
    data.at[i, 'numOpenPorts'] = hf.numOpenPorts()

data.to_csv('ProcessedData.csv', encoding='utf-8')